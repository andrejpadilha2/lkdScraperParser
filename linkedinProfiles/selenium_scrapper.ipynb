{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from time import sleep\n",
    "from timeit import default_timer as timer\n",
    "from datetime import datetime\n",
    "\n",
    "from selenium_stealth import stealth\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "from utils.generate_name_variations import generate_name_variations\n",
    "from utils.methods import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_studied_at_universities(page_source, universities_to_check):\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    # Find the script tag containing the JSON-LD data\n",
    "    script = soup.find('script', type='application/ld+json')\n",
    "\n",
    "    if script:\n",
    "        # Extract the JSON content\n",
    "        json_data = script.string\n",
    "        print(f\"json data: {json_data}\")\n",
    "\n",
    "        # Parse the JSON content\n",
    "        data = json.loads(json_data)\n",
    "        print(f\"data: {data}\")\n",
    "\n",
    "        try:\n",
    "            # Check if the person studied at any of the specified universities\n",
    "            alumni_of = data['@graph'][0]['alumniOf']\n",
    "            print(f\"alumni of: {alumni_of}\")\n",
    "            universities_studied = [org['name'] for org in alumni_of if org['@type'] == 'EducationalOrganization']\n",
    "            print(f\"universities: {universities_studied}\")\n",
    "\n",
    "            for university_to_check in universities_to_check:\n",
    "                for university_studied in universities_studied:\n",
    "                    if university_to_check in university_studied:\n",
    "                        return True\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_page_problems(page_source):\n",
    "    problems = \"\"\n",
    "    success = 1\n",
    "\n",
    "    if \"authwall\" in page_source:\n",
    "        print(\"→ You hit the authentication wall!\")\n",
    "        problems = \"authwall_\"\n",
    "        success = 0\n",
    "\n",
    "    if \"captcha\" in page_source:\n",
    "        print(\"→ You hit a captcha page!\")\n",
    "        problems += \"captcha_\"\n",
    "        success = 0\n",
    "\n",
    "    if page_source.startswith(\"<html><head>\\n    <script type=\\\"text/javascript\\\">\\n\"):\n",
    "        print(\"→ You hit javascript obfuscated code!\")\n",
    "        problems += \"obfuscatedJS_\"\n",
    "        success = 0\n",
    "    \n",
    "    return problems, success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_webdriver():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"start-maximized\")\n",
    "    options.add_argument('--blink-settings=imagesEnabled=false')\n",
    "    # options.add_argument(\"--headless\")\n",
    "    options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    options.add_experimental_option('useAutomationExtension', False)\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    driver.implicitly_wait(60)\n",
    "    stealth(driver,\n",
    "            languages=[\"en-US\", \"en\"],\n",
    "            vendor=\"Google Inc.\",\n",
    "            platform=\"Win32\",\n",
    "            webgl_vendor=\"Intel Inc.\",\n",
    "            renderer=\"Intel Iris OpenGL Engine\",\n",
    "            fix_hairline=True)\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ sleeping 1 second...\n",
      "→ Requesting 'www.google.com.br'.\n",
      "→ sleeping between 1 and 2 seconds...\n",
      "→ Searching on Google.\n",
      "→ sleeping between 2 and 3 seconds...\n",
      "['joao', 'caprera']\n",
      "['joao', 'caprera']\n",
      "True\n",
      "→ Requesting 'https://br.linkedin.com/in/joao-caprera-7718432b'.\n",
      "→ sleeping between 5 and 7 seconds...\n"
     ]
    }
   ],
   "source": [
    "full_name = 'João Caprera' # André José de Queiroz Padilha, Gabriela Bertoni dos Santos, João Caprera, Caio Fiaschi da Silva\n",
    "name_variation = 'João Caprera'\n",
    "\n",
    "\n",
    "driver = initialize_webdriver()\n",
    "sleep(1, '→ sleeping 1 second...')\n",
    "\n",
    "print(\"→ Requesting 'www.google.com.br'.\")\n",
    "driver.get('https://www.google.com.br')\n",
    "sleep(random.uniform(1, 2), '→ sleeping between 1 and 2 seconds...')\n",
    "\n",
    "search_box = driver.find_element(By.NAME, 'q')\n",
    "search_query = f\"{name_variation} ufabc linkedin\"\n",
    "print(\"→ Searching on Google.\")\n",
    "search_box.send_keys(search_query)\n",
    "search_box.send_keys(Keys.RETURN)\n",
    "sleep(random.uniform(2, 3), '→ sleeping between 2 and 3 seconds...')\n",
    "links = driver.find_elements(By.TAG_NAME, 'a')\n",
    "linkedin_links = [link for link in links if link.get_attribute('href') and 'linkedin.com/in/' in link.get_attribute('href')]\n",
    "\n",
    "if linkedin_links:               \n",
    "    linkedin_url = linkedin_links[0].get_attribute('href').split(\"?\")[0] # we only consider the first linkedin profile\n",
    "\n",
    "    linkedin_link_title = linkedin_links[0].text.split()\n",
    "    hyphen_index = linkedin_link_title.index('-')\n",
    "\n",
    "    names_in_linkedin_link = linkedin_link_title[:hyphen_index]\n",
    "    names_in_linkedin_link = [normalize_string(name) for name in names_in_linkedin_link]\n",
    "    print(names_in_linkedin_link)\n",
    "\n",
    "    names_in_full_name = full_name.split()\n",
    "    names_in_full_name = [normalize_string(name) for name in names_in_full_name]\n",
    "    print(names_in_full_name)\n",
    "\n",
    "    is_subset = set(names_in_linkedin_link) <= set(names_in_full_name)\n",
    "    print(is_subset)\n",
    "\n",
    "    first_name_name_variation = normalize_string(name_variation.split()[0].lower())\n",
    "\n",
    "    if not is_subset:\n",
    "        print(f\"→ Names of Linkedin profile are not a subset of real full name, skipping...\")\n",
    "    else:\n",
    "        print(f\"→ Requesting '{linkedin_url}'.\")\n",
    "        linkedin_links[0].click()\n",
    "        sleep(random.uniform(5, 7), '→ sleeping between 5 and 7 seconds...')\n",
    "\n",
    "        page_source = driver.page_source\n",
    "        problems, success = check_page_problems(page_source) \n",
    "        # TODO\n",
    "        # If it's the xth unsucessful reply in a row, do something\n",
    "        driver.close()      \n",
    "else:\n",
    "    print(\"→ No LinkedIn search results to access.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role: Analista de Risco Operacional Pleno\n",
      "Description: WMS - Investment Service Operations\n",
      "Location: None\n",
      "Company: Itaú Unibanco\n",
      "Start Time: dez. de 2022\n",
      "End Time: Ongoing\n",
      "Duration: 7 meses\n",
      "\n",
      "Role: Analista de Risco Operacional Pleno\n",
      "Description: Gestão de Continuidade de Negócios\n",
      "Location: None\n",
      "Company: Itaú Unibanco\n",
      "Start Time: set. de 2021\n",
      "End Time: dez. de 2022\n",
      "Duration: 1 ano 4 meses\n",
      "\n",
      "Role: Analista de Risco Operacional Jr.\n",
      "Description: Riso Operacional:- Desenvolvimento e manutenção de ferramentas- Implementação de metodologia de análise de risco operacional\n",
      "Location: São Paulo e Região, Brasil\n",
      "Company: Itaú Unibanco\n",
      "Start Time: dez. de 2017\n",
      "End Time: set. de 2021\n",
      "Duration: 3 anos 10 meses\n",
      "\n",
      "Role: Estagiário\n",
      "Description: Controle Interno de Risco Operacional- Análise do negócio \"Veículos\"- Monitoramento de ocorrências e riscos das operações\n",
      "Location: None\n",
      "Company: Itaú Unibanco\n",
      "Start Time: mar. de 2017\n",
      "End Time: dez. de 2017\n",
      "Duration: 10 meses\n",
      "\n",
      "Role: Lean Enterprise Summer Program\n",
      "Description: - Programa de verão voltado ao aperfeiçoamento de técnicas e metodologias Lean- Projeto de análise de operações na Carlex Glass of America LLC\n",
      "Location: Knoxville, Tennessee e Região, Estados Unidos\n",
      "Company: University of Tennessee\n",
      "Start Time: jun. de 2016\n",
      "End Time: jul. de 2016\n",
      "Duration: 2 meses\n",
      "\n",
      "Role: Iniciação Científica - Pesquisador\n",
      "Description: - Iniciação científica supervisionada pelo CNPq entitulado \"Estudo de nanopartículas magnéticas aplicadas no tratamento de câncer por hiperthermia.''\n",
      "Location: Santo André - Brazil\n",
      "Company: Universidade Federal do ABC - UFABC\n",
      "Start Time: ago. de 2014\n",
      "End Time: ago. de 2015\n",
      "Duration: 1 ano 1 mês\n",
      "\n",
      "University: ['Universidade Federal do ABC - UFABC', 'Universidade Federal do ABC - UFABC', 'University of Southern California', 'Colégio São Miguel Arcanjo']\n",
      "Degree Info: [['Engenharia de Gestão'], ['Bacharelado em Ciência e Tecnologia'], ['Bacharelado em Engenharia', 'Engenharia Industrial'], ['Colegial']]\n",
      "Duration: ['2013 - 2019', '2013 - 2017', '2015 - 2016', '2010 - 2012']\n",
      "Details: [None, None, None, None]\n",
      "Activities and groups [None, None, None, None]\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "def extract_education_info(page_source):\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    education_items = soup.find_all('li', class_='education__list-item')\n",
    "\n",
    "    universities = []\n",
    "    degrees = []\n",
    "    durations = []\n",
    "    details_list = []\n",
    "    activities_groups_list = []\n",
    "\n",
    "    for item in education_items:\n",
    "        university_element = item.find('h3', class_='profile-section-card__title')\n",
    "        university = university_element.text.strip() if university_element else None\n",
    "        universities.append(university)\n",
    "        \n",
    "        degree_info_elements = item.find_all('span', class_='education__item--degree-info')\n",
    "        degree_info = [degree_info_element.text.strip() for degree_info_element in degree_info_elements]\n",
    "        degrees.append(degree_info)\n",
    "        \n",
    "        duration_element = item.find('p', class_='education__item--duration')\n",
    "        duration = duration_element.text.strip() if duration_element else None\n",
    "        durations.append(duration)\n",
    "\n",
    "        details_element = item.find('div', class_='show-more-less-text')\n",
    "        details = details_element.text.strip() if details_element else None\n",
    "        details_list.append(details)\n",
    "\n",
    "        activities_groups_element = item.find('p', class_='education__item--activities-and-societies')\n",
    "        activities_groups = activities_groups_element.text.strip() if activities_groups_element else None\n",
    "        activities_groups_list.append(activities_groups)\n",
    "\n",
    "    return universities, degrees, durations, details_list, activities_groups_list\n",
    "\n",
    "def extract_professional_experience(page_source):\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    # Find all experience items\n",
    "    experience_list = soup.find('ul', class_='experience__list')\n",
    "    experience_items = experience_list.find_all('li', class_='profile-section-card')\n",
    "\n",
    "    # Iterate over each experience item\n",
    "    for item in experience_items:\n",
    "        # Extract the role name\n",
    "        role_name = item.find('h3', class_='profile-section-card__title').text.strip()\n",
    "\n",
    "        # Extract the description (if available)\n",
    "        description = None\n",
    "        description_element = item.find('div', class_='experience-item__description')\n",
    "        if not description_element:\n",
    "            description_element = item.find('div', class_='experience-group-position__description')\n",
    "        \n",
    "        if description_element:\n",
    "            description_big = description_element.find('p', class_='show-more-less-text__text--more')\n",
    "            if description_big:\n",
    "                description = description_big.get_text(strip=True).replace('Exibir menos', '')\n",
    "            else:\n",
    "                description = description_element.find('p', class_='show-more-less-text__text--less').get_text(strip=True)\n",
    "\n",
    "        # Extract the location (if available)\n",
    "        location = None\n",
    "        location_element = item.find('p', class_='experience-item__location')\n",
    "        if not location_element:\n",
    "            location_element = item.find('p', class_='experience-group-position__location')\n",
    "        if location_element:\n",
    "            location = location_element.text.strip()\n",
    "\n",
    "\n",
    "        # Extract the company name (if available)\n",
    "        company_element = item.find('h4', class_='profile-section-card__subtitle')\n",
    "        company_name = company_element.text.strip() if company_element else None\n",
    "\n",
    "        # Find the date range element\n",
    "        date_range_element = item.find('span', class_='date-range')\n",
    "\n",
    "        # Extract the start time, end time, and duration\n",
    "        if date_range_element:\n",
    "            time_elements = date_range_element.find_all('time')\n",
    "            duration_element = date_range_element.find('span', class_='before:middot')\n",
    "\n",
    "            if time_elements and len(time_elements) >= 1:\n",
    "                start_time = time_elements[0].text.strip()\n",
    "\n",
    "                if len(time_elements) == 2:\n",
    "                    end_time = time_elements[1].text.strip()\n",
    "                else:\n",
    "                    end_time = \"Ongoing\"\n",
    "            else:\n",
    "                start_time = None\n",
    "                end_time = None\n",
    "\n",
    "            if duration_element:\n",
    "                duration = duration_element.text.strip()\n",
    "            else:\n",
    "                duration = None\n",
    "\n",
    "        # Print the extracted information\n",
    "        print(\"Role:\", role_name)\n",
    "        print(\"Description:\", description)\n",
    "        print(\"Location:\", location)\n",
    "        print(\"Company:\", company_name)\n",
    "        print(\"Start Time:\", start_time)\n",
    "        print(\"End Time:\", end_time)\n",
    "        print(\"Duration:\", duration)\n",
    "        print()\n",
    "\n",
    "\n",
    "if success:\n",
    "    extract_professional_experience(page_source)\n",
    "\n",
    "if success:\n",
    "    universities, degrees, durations, details_list, activities_groups_list = extract_education_info(page_source)\n",
    "\n",
    "    # Print the extracted information\n",
    "    print('University:', universities)\n",
    "    print('Degree Info:', degrees)\n",
    "    print('Duration:', durations)\n",
    "    print('Details:', details_list)\n",
    "    print('Activities and groups', activities_groups_list)\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "linkedin_scrapper",
   "language": "python",
   "name": "linkedin_scrapper"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "835a58a9326911df0cc25148838ba18195d724fb2eaa1b48a6b9900a4baac6e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
